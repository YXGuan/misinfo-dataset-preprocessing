# -*- coding: utf-8 -*-
"""Misinfo_veracityMapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UY1Zl6BKUmHd4t-gTIzZFjqjTS3Uwvu4

I would process everything in DASK
"""

!pip install dask pyarrow requests fsspec

!pip install dask[dataframe]

import dask.dataframe as dd
import requests
import io

# Step 2: Load the Parquet file into a Dask DataFrame
# Dask can read from a file-like object using the `getpyarrowfilesystem` option
dask_df = dd.read_parquet("/content/dat_claims_veracity_mapped_moved_converted_updatedSplit.parquet", engine='pyarrow')

# Step 3: Map the values in the "veracity" column
dask_df['veracity'] = dask_df['veracity'].map({'True': 'true', 'False': 'false', 'Na' : 'Na', 'true' : 'true', 'false': 'false', 'unknown': 'unknown'})

# Step 4: Save the modified DataFrame back to a Parquet file
output_path = "modified_train.parquet"
dask_df.to_parquet(output_path)

print(f"Modified Parquet file saved to {output_path}")

import dask.dataframe as dd

# Read multiple Parquet files into a Dask DataFrame
ddf = dd.read_parquet("/content/modified_train.parquet/*.parquet")

# Write the Dask DataFrame as a single Parquet file
ddf.repartition(npartitions=1).to_parquet("/content/modified_train.parquet/single_parquet_file.parquet")

import pandas as pd
import dask.dataframe as dd

# Define the file paths
original_file_path = "/content/dat_claims_veracity_mapped_moved_converted_updatedSplit.parquet"
modified_file_path = "/content/modified_train.parquet/single_parquet_file.parquet/part.0.parquet"

# Read the original Parquet file using pandas
original_df = pd.read_parquet(original_file_path)

# Read the modified Parquet file using Dask
dask_df = dd.read_parquet(modified_file_path, engine='pyarrow')
modified_df = dask_df.compute()  # Convert Dask DataFrame to Pandas DataFrame for easier comparison

# Sample the "veracity" column from both DataFrames
print("Original 'veracity' column sample:")
print(original_df['veracity'].value_counts())

print("\nModified 'veracity' column sample:")
print(modified_df['veracity'].value_counts())

# Verification: Check the mapping
def verify_mapping(original_df, modified_df):
    # Verify that 'True' has been changed to 'true' and 'False' to 'false'
    # if not original_df['veracity'].isin(['True', 'False']).all():
    #     print("Error: Original 'veracity' column contains unexpected values.")
    #     return

    correct_mapping = {
        'True': 'true',
        'False': 'false'
    }

    for original_value, mapped_value in correct_mapping.items():
        orig_count = (original_df['veracity'] == original_value).sum()
        mod_count = (modified_df['veracity'] == mapped_value).sum()
        if orig_count != mod_count:
            print(f"Mismatch found: {original_value} (original) and {mapped_value} (mapped)")
            return

    print("Verification successful: 'veracity' column has been correctly mapped.")

# Run the verification
verify_mapping(original_df, modified_df)